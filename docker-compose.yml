services:

  #-----------------------------
  # API SERVICE
  #-----------------------------
  # This is the real API available at port 3030.  It builds an image 
  # for the top level docker-compose and may not be the same image 
  # as built under the /api folder directly
  api:
    extends:
      file: ./api/docker-compose.yml
      service: api
    profiles:
      - backend
      - local-fullstack
      - fullstack
      - ''

  #-----------------------------
  # DATABASE SERVICE
  #-----------------------------
  # This is the postgres docker DB available at port 35432 for local usage.
  # When referencing the db from a compose container, use database:5432
  # When referencing the db from an external process, use localhost:35432
  database:    
    extends:
      file: ./api/docker-compose.yml
      service: database
    profiles:
      - backend
      - local-fullstack
      - fullstack
      - ''

  #-----------------------------
  # CLOUD STORAGE MOCK SERVICE
  #-----------------------------
  # This mocks out a Google Cloud Storage server so that local testing
  # can be done with a GCS Bucket library.
  storage:
    extends:
      file: ./storage/docker-compose.yml
      service: storage
    profiles:
      - backend
      - local-fullstack
      - fullstack
      - ''

  #-----------------------------
  # NLM PARSER SERVICE
  #-----------------------------
  # This Sping up an NLMParser service with a bundled Apache Tika server instance
  parser-nlm:
    extends:
      file: ./parsers/docker-compose.yml
      service: nlm-ingestor
    profiles:
      - backend
      - local-fullstack
      - fullstack
      - ''
  #-----------------------------
  # WEB UI SERVICE
  #-----------------------------
  # This is the UI service that is typically deployed at the root
  # of the domain for this project. In the cloud, this should be served 
  # by a static proxy.
  interface-web:
    extends:
      file: ./interfaces/web/docker-compose.yml
      service: web
    profiles:
      - frontend
      - local-fullstack
      - fullstack
      - ''

  #-----------------------------
  # LOCAL LLM SERVICE
  #-----------------------------
  # This is an instance of an Ollama docker container that runs entirely locally.
  # This can be done to test the LLM functionality locally without need for a cloud API key.
  # It is significantly slower than the cloud version, but can be GPU enabled on some platforms.
  local-llm:
    extends:
      file: ./local-llm/docker-compose.yml
      service: ollama
    profiles:
      - local-fullstack

networks:
  madi:

volumes:
  database-data:
  ollama:
